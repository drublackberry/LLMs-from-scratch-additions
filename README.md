# Additions to "Build a Large Language Model (from Scratch)"
This is a collection of files that add to the fantastic book ["Build a Large Language Model (from Scratch)](https://github.com/rasbt/LLMs-from-scratch) from Sebastian Raschka 

# Index

You can find the following additions in this repo:

- [Learnings](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/learnings.md). These are side notes taken by myself and help me understand with more depth certain concepts.
- [Attention sizes](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/attention-sizes.md). This is a clarifying guide tothe dimensions of the matrices in the attention mechanism.
- [Cross-attention](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/cross-attention.md). This explains the cross-attention mechanism, which is not covered in the book.
- [RLHF](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/rlhf.md). This explains Reinforcement Learning from Human Feedback, a crucial technique for aligning LLMs.
- [DPO](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/dpo.md). This explains Direct Preference Optimization, a simpler and more efficient alternative to RLHF.
- [GRPO](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/grpo.md). This explains Group Relative Policy Optimization, an even more advanced and efficient method for preference optimization.
- [Perplexity](https://github.com/drublackberry/LLMs-from-scratch-additions/blob/main/perplexity.md). This explains the perplexity metric, which is used to evaluate language models.